{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0ff975",
   "metadata": {},
   "source": [
    "# Text based\n",
    "- add label to text\n",
    "- set anchor text\n",
    "- cosine similarity\n",
    "- cross validation/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c3a4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a03e0",
   "metadata": {},
   "source": [
    "## Gentrified Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95883d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Geographic 2D CRS: EPSG:4326>\n",
       "Name: WGS 84\n",
       "Axis Info [ellipsoidal]:\n",
       "- Lat[north]: Geodetic latitude (degree)\n",
       "- Lon[east]: Geodetic longitude (degree)\n",
       "Area of Use:\n",
       "- name: World.\n",
       "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
       "Datum: World Geodetic System 1984 ensemble\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the gentri label to the vectorization text\n",
    "# use 'polygon' or 'wgs84_polygon', depend on what coordinate the label is\n",
    "# load gentri label\n",
    "lsoa_label = gpd.read_file(\"data/gentri_data/london_gentri_labeled.shp\")\n",
    "lsoa_label.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d14fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['site_name', 'decision_date', 'valid_date', 'polygon.geometries',\n",
      "       'polygon.type', 'wgs84_polygon.coordinates', 'wgs84_polygon.type',\n",
      "       'description', 'borough', 'street_name',\n",
      "       ...\n",
      "       'sbert_374', 'sbert_375', 'sbert_376', 'sbert_377', 'sbert_378',\n",
      "       'sbert_379', 'sbert_380', 'sbert_381', 'sbert_382', 'sbert_383'],\n",
      "      dtype='object', length=401)\n"
     ]
    }
   ],
   "source": [
    "# use the wgs84_polygon to join the label with text\n",
    "# read the text\n",
    "text = pd.read_parquet(\"sbert_encoded_data.parquet\")\n",
    "print(text.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d528cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn polygon coordinate into shapely \n",
    "# make sure to drop the null geodata\n",
    "text_valid = text[text['wgs84_polygon.coordinates'].notnull()].copy()\n",
    "text_valid[\"geometry\"] = text_valid[\"wgs84_polygon.coordinates\"].apply(lambda coords: Polygon(coords[0]))\n",
    "\n",
    "# creat GeoDataFrameï¼Œ set WGS84 coordinate\n",
    "gdf_text = gpd.GeoDataFrame(text_valid, geometry=\"geometry\", crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac8c1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = gpd.sjoin(gdf_text, lsoa_label[[\"geometry\", \"gentrified\"]], how=\"left\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "646aad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the percentage of texts being labeled: 0.8843049615743271\n"
     ]
    }
   ],
   "source": [
    "print(\"the percentage of texts being labeled:\", joined['gentrified'].notna().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859986ee",
   "metadata": {},
   "source": [
    "Noramlly all the text should be labeled, so this might have two causes.\n",
    "- set the wrong coordinate\n",
    "- text's polygon cross more than one lsoa  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e5609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2     POINT (-0.20623 51.57753)\n",
      "9     POINT (-0.10164 51.51549)\n",
      "15    POINT (-0.09637 51.53113)\n",
      "19    POINT (-0.10233 51.53794)\n",
      "60    POINT (-0.12107 51.56635)\n",
      "dtype: geometry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_29404\\2548137795.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  print(gdf_unmatched.geometry.centroid.head())\n"
     ]
    }
   ],
   "source": [
    "# check the geometry\n",
    "gdf_unmatched = joined[joined['gentrified'].isna()]\n",
    "gdf_unmatched = gdf_unmatched.to_crs(\"EPSG:4326\")\n",
    "print(gdf_unmatched.geometry.centroid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9378b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total texts: 155495\n",
      "text within lsoa: 137505\n"
     ]
    }
   ],
   "source": [
    "# check the texts\n",
    "print(\"total texts:\", len(gdf_text))\n",
    "print(\"text within lsoa:\", joined['gentrified'].notna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea2693",
   "metadata": {},
   "source": [
    "Geometry seems fine, and not all texts are within lsoas, meaning some text polygon cross over more than one lsoas.\n",
    "\n",
    "Two solutions:\n",
    "1. drop the texts that cross several lsoas\n",
    "2. change within to intersects, but this would lead to one text have more than one label\n",
    "\n",
    "For now i choose to drop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7192f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_labeled = joined.dropna(subset=['gentrified']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57fced57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "gentrified",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "proportion",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6055caff-762a-45b0-b0b2-0f6d2dbe823a",
       "rows": [
        [
         "False",
         "0.7864877640813062"
        ],
        [
         "True",
         "0.21351223591869387"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "gentrified\n",
       "False    0.786488\n",
       "True     0.213512\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_labeled['gentrified'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a7047",
   "metadata": {},
   "source": [
    "The percentage of gentrified texts (0.21) are similar with the gentrified lsoas (0.24).\n",
    "Here I tried a baseline model (without anchor text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f81f53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['site_name', 'decision_date', 'valid_date', 'polygon.geometries',\n",
       "       'polygon.type', 'wgs84_polygon.coordinates', 'wgs84_polygon.type',\n",
       "       'description', 'borough', 'street_name',\n",
       "       ...\n",
       "       'sbert_377', 'sbert_378', 'sbert_379', 'sbert_380', 'sbert_381',\n",
       "       'sbert_382', 'sbert_383', 'geometry', 'index_right', 'gentrified'],\n",
       "      dtype='object', length=404)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_labeled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79e6dfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.63      0.71     21629\n",
      "           1       0.27      0.52      0.36      5872\n",
      "\n",
      "    accuracy                           0.60     27501\n",
      "   macro avg       0.55      0.57      0.54     27501\n",
      "weighted avg       0.71      0.60      0.64     27501\n",
      "\n",
      "AUC Score: 0.6088463358370781\n"
     ]
    }
   ],
   "source": [
    "# convert the SBERT list vectors into numpy arrays \n",
    "# select all SBERT columns\n",
    "sbert_cols = [col for col in joined_labeled.columns if col.startswith(\"sbert_\")]\n",
    "\n",
    "# combine these columns into a matrix X (with N rows and 384 columns)\n",
    "X = joined_labeled[sbert_cols].values  # shape = (N, 384)\n",
    "y = joined_labeled[\"gentrified\"].astype(int).values\n",
    "\n",
    "# set train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "# initialize the model and handle the imbalance using class_weight\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# prediction and evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4cb9e",
   "metadata": {},
   "source": [
    "The model has some predicting capabilities (with an AUC value slightly over 0.6), but its ability to predict gentrified = 1 is still insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce26e7bc",
   "metadata": {},
   "source": [
    "## Anchor texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
