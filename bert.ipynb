{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57736916",
   "metadata": {},
   "source": [
    "Just wanting to see what would happen if we use fine-tuned bert itself to find out the topic rather then we defined anchor text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87f01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba6b1edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Projected CRS: EPSG:27700>\n",
       "Name: OSGB36 / British National Grid\n",
       "Axis Info [cartesian]:\n",
       "- E[east]: Easting (metre)\n",
       "- N[north]: Northing (metre)\n",
       "Area of Use:\n",
       "- name: United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\n",
       "- bounds: (-9.01, 49.75, 2.01, 61.01)\n",
       "Coordinate Operation:\n",
       "- name: British National Grid\n",
       "- method: Transverse Mercator\n",
       "Datum: Ordnance Survey of Great Britain 1936\n",
       "- Ellipsoid: Airy 1830\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the gentri label to the vectorization text\n",
    "# use 'polygon' or 'wgs84_polygon', depend on what coordinate the label is\n",
    "# load gentri label and description\n",
    "df_london_all3 = pd.read_csv(\"data/cleaned_projects_raw.csv\")\n",
    "lsoa_label = gpd.read_file(\"data/gentri_data/london_gentri_labeled_25.shp\")\n",
    "lsoa_label.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c583dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn polygon coordinate into shapely \n",
    "# make sure to drop the null geodata\n",
    "text_valid = df_london_all3[\n",
    "    df_london_all3['wgs84_polygon.coordinates'].notnull() & \n",
    "    (df_london_all3['wgs84_polygon.type'] == 'Polygon')\n",
    "].copy()\n",
    "text_valid[\"geometry\"] = text_valid[\"wgs84_polygon.coordinates\"].apply(\n",
    "    lambda coords: Polygon(ast.literal_eval(coords)[0])\n",
    ")\n",
    "# creat GeoDataFrame， set WGS84 coordinate\n",
    "gdf_text = gpd.GeoDataFrame(text_valid, geometry=\"geometry\", crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b85c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_27700 = gdf_text.to_crs(\"EPSG:27700\")\n",
    "# get the centroid of each polygon\n",
    "gdf_27700[\"centroid\"] = gdf_27700.geometry.centroid\n",
    "joined_centroid = gpd.sjoin(gdf_27700.set_geometry(\"centroid\"), lsoa_label[[\"geometry\", \"gentrified\"]],how=\"left\",predicate=\"within\")\n",
    "joined_cleaned = joined_centroid[joined_centroid[\"gentrified\"].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cefde35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_name</th>\n",
       "      <th>decision_date</th>\n",
       "      <th>valid_date</th>\n",
       "      <th>polygon.geometries</th>\n",
       "      <th>polygon.type</th>\n",
       "      <th>wgs84_polygon.coordinates</th>\n",
       "      <th>wgs84_polygon.type</th>\n",
       "      <th>description</th>\n",
       "      <th>borough</th>\n",
       "      <th>street_name</th>\n",
       "      <th>total_no_proposed_residential_units</th>\n",
       "      <th>total_no_existing_residential_units</th>\n",
       "      <th>polygon</th>\n",
       "      <th>wgs84_polygon</th>\n",
       "      <th>polygon.coordinates</th>\n",
       "      <th>year</th>\n",
       "      <th>geometry</th>\n",
       "      <th>centroid</th>\n",
       "      <th>index_right</th>\n",
       "      <th>gentrified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-04-17</td>\n",
       "      <td>2015-02-20</td>\n",
       "      <td>[]</td>\n",
       "      <td>geometrycollection</td>\n",
       "      <td>[[[-0.330153, 51.480892], [-0.330154, 51.48086...</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>Erection of side and rear extension to existin...</td>\n",
       "      <td>Hounslow</td>\n",
       "      <td>Roxborough Avenue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>POLYGON ((516053.478 177092.461, 516053.477 17...</td>\n",
       "      <td>POINT (516051.984 177090.981)</td>\n",
       "      <td>2620.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-12</td>\n",
       "      <td>2015-01-19</td>\n",
       "      <td>[{'coordinates': [[[532400.5928219, 185252.991...</td>\n",
       "      <td>GeometryCollection</td>\n",
       "      <td>[[[-0.0917876, 51.5506417], [-0.0918015, 51.55...</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>Side extension at basement, rebuild steps, new...</td>\n",
       "      <td>Islington</td>\n",
       "      <td>Highbury New Park</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>POLYGON ((532402.417 185253.256, 532401.522 18...</td>\n",
       "      <td>POINT (532395.532 185252.542)</td>\n",
       "      <td>2716.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-07-27</td>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>[]</td>\n",
       "      <td>geometrycollection</td>\n",
       "      <td>[[[-0.414458, 51.439358], [-0.414459, 51.43933...</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>Certificate of Lawfulness for the erection of ...</td>\n",
       "      <td>Hounslow</td>\n",
       "      <td>Parkfield Road</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>POLYGON ((510299.53 172343.489, 510299.525 172...</td>\n",
       "      <td>POINT (510298.015 172342.015)</td>\n",
       "      <td>2556.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-30</td>\n",
       "      <td>2015-10-16</td>\n",
       "      <td>[{'coordinates': [[[530957.946236, 184135.3910...</td>\n",
       "      <td>GeometryCollection</td>\n",
       "      <td>[[[-0.1129983, 51.5409351], [-0.1130422, 51.54...</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>Erection of single storey timber clad garden r...</td>\n",
       "      <td>Islington</td>\n",
       "      <td>Hemingford Road</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>POLYGON ((530959.741 184135.694, 530956.693 18...</td>\n",
       "      <td>POINT (530946.671 184138.641)</td>\n",
       "      <td>2639.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>2015-07-28</td>\n",
       "      <td>2015-06-17</td>\n",
       "      <td>[{'coordinates': [[[521146.4472015, 180763.396...</td>\n",
       "      <td>GeometryCollection</td>\n",
       "      <td>[[[-0.2555649, 51.5128182], [-0.2556564, 51.51...</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>Single storey (max 6m deep and max 3.875m high...</td>\n",
       "      <td>Ealing</td>\n",
       "      <td>Bowes Road</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>POLYGON ((521148.041 180763.566, 521141.701 18...</td>\n",
       "      <td>POINT (521140.311 180783.762)</td>\n",
       "      <td>1222.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  site_name decision_date  valid_date  \\\n",
       "0       NaN    2015-04-17  2015-02-20   \n",
       "1       NaN    2015-03-12  2015-01-19   \n",
       "2       NaN    2015-07-27  2015-06-01   \n",
       "3       NaN    2015-11-30  2015-10-16   \n",
       "4        20    2015-07-28  2015-06-17   \n",
       "\n",
       "                                  polygon.geometries        polygon.type  \\\n",
       "0                                                 []  geometrycollection   \n",
       "1  [{'coordinates': [[[532400.5928219, 185252.991...  GeometryCollection   \n",
       "2                                                 []  geometrycollection   \n",
       "3  [{'coordinates': [[[530957.946236, 184135.3910...  GeometryCollection   \n",
       "4  [{'coordinates': [[[521146.4472015, 180763.396...  GeometryCollection   \n",
       "\n",
       "                           wgs84_polygon.coordinates wgs84_polygon.type  \\\n",
       "0  [[[-0.330153, 51.480892], [-0.330154, 51.48086...            Polygon   \n",
       "1  [[[-0.0917876, 51.5506417], [-0.0918015, 51.55...            Polygon   \n",
       "2  [[[-0.414458, 51.439358], [-0.414459, 51.43933...            Polygon   \n",
       "3  [[[-0.1129983, 51.5409351], [-0.1130422, 51.54...            Polygon   \n",
       "4  [[[-0.2555649, 51.5128182], [-0.2556564, 51.51...            Polygon   \n",
       "\n",
       "                                         description    borough  \\\n",
       "0  Erection of side and rear extension to existin...   Hounslow   \n",
       "1  Side extension at basement, rebuild steps, new...  Islington   \n",
       "2  Certificate of Lawfulness for the erection of ...   Hounslow   \n",
       "3  Erection of single storey timber clad garden r...  Islington   \n",
       "4  Single storey (max 6m deep and max 3.875m high...     Ealing   \n",
       "\n",
       "         street_name  total_no_proposed_residential_units  \\\n",
       "0  Roxborough Avenue                                  NaN   \n",
       "1  Highbury New Park                                  NaN   \n",
       "2     Parkfield Road                                  NaN   \n",
       "3    Hemingford Road                                  NaN   \n",
       "4         Bowes Road                                  NaN   \n",
       "\n",
       "   total_no_existing_residential_units  polygon  wgs84_polygon  \\\n",
       "0                                  NaN      NaN            NaN   \n",
       "1                                  NaN      NaN            NaN   \n",
       "2                                  NaN      NaN            NaN   \n",
       "3                                  NaN      NaN            NaN   \n",
       "4                                  NaN      NaN            NaN   \n",
       "\n",
       "  polygon.coordinates  year  \\\n",
       "0                 NaN  2015   \n",
       "1                 NaN  2015   \n",
       "2                 NaN  2015   \n",
       "3                 NaN  2015   \n",
       "4                 NaN  2015   \n",
       "\n",
       "                                            geometry  \\\n",
       "0  POLYGON ((516053.478 177092.461, 516053.477 17...   \n",
       "1  POLYGON ((532402.417 185253.256, 532401.522 18...   \n",
       "2  POLYGON ((510299.53 172343.489, 510299.525 172...   \n",
       "3  POLYGON ((530959.741 184135.694, 530956.693 18...   \n",
       "4  POLYGON ((521148.041 180763.566, 521141.701 18...   \n",
       "\n",
       "                        centroid  index_right gentrified  \n",
       "0  POINT (516051.984 177090.981)       2620.0      False  \n",
       "1  POINT (532395.532 185252.542)       2716.0      False  \n",
       "2  POINT (510298.015 172342.015)       2556.0      False  \n",
       "3  POINT (530946.671 184138.641)       2639.0      False  \n",
       "4  POINT (521140.311 180783.762)       1222.0      False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e69e5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = joined_cleaned[['description', 'gentrified']].dropna()\n",
    "df = df.rename(columns={'description': 'text', 'gentrified': 'label'})\n",
    "df['label'] = df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd474bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fcc9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d075218d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4b2d236d764e1b954d8570453c47cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123645 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5e37bd27c84877840a61f0d632d28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30912 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eea27c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5383a3298cc543848834c36014b9b531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100b0947205e41d0a79de34bdfa1d941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels),\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"binary\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44346775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_gentri_output\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    max_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c2b01d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_19828\\211151094.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 1:24:10, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.138600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.19711716365814208, metrics={'train_runtime': 5056.6879, 'train_samples_per_second': 0.791, 'train_steps_per_second': 0.099, 'total_flos': 529869594624000.0, 'train_loss': 0.19711716365814208, 'epoch': 4.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "small_train = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e993c8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2532597780227661,\n",
       " 'eval_accuracy': {'accuracy': 0.945},\n",
       " 'eval_f1': {'f1': 0.0},\n",
       " 'eval_runtime': 26.1812,\n",
       " 'eval_samples_per_second': 7.639,\n",
       " 'eval_steps_per_second': 0.955,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcc701a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "labels = df['label']\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3e4746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class WeightedDistilBert(DistilBertForSequenceClassification):\n",
    "    def __init__(self, config, class_weights):\n",
    "        super().__init__(config)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask, labels=None, **kwargs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        else:\n",
    "            return {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f804cf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WeightedDistilBert were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertConfig\n",
    "\n",
    "config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model = WeightedDistilBert.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels=2,\n",
    "    class_weights=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bba2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_gentri_output\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    max_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d6648e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_19828\\2724630097.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "e:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'num_items_in_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      2\u001b[39m small_eval = tokenized_dataset[\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m].shuffle(seed=\u001b[32m42\u001b[39m).select(\u001b[38;5;28mrange\u001b[39m(\u001b[32m200\u001b[39m))\n\u001b[32m      4\u001b[39m trainer = Trainer(\n\u001b[32m      5\u001b[39m     model=model,\n\u001b[32m      6\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\transformers\\trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\transformers\\trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\transformers\\trainer.py:3749\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3746\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3749\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3751\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3752\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3753\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3754\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3755\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\transformers\\trainer.py:3836\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3834\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3835\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3836\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3837\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3838\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3839\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mWeightedDistilBert.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, labels, **kwargs)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids=\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask=\u001b[38;5;28;01mNone\u001b[39;00m, labels=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     logits = outputs.logits\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'num_items_in_batch'"
     ]
    }
   ],
   "source": [
    "small_train = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
