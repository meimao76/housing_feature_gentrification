{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fcb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_10_13 = pd.read_csv('data/cleaned_projects_raw_10_13.csv')\n",
    "df_14 = pd.read_csv('data/cleaned_projects_raw_14.csv')\n",
    "df_19 = pd.read_csv('data/cleaned_projects_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd09a702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227633, 16)\n"
     ]
    }
   ],
   "source": [
    "df_10_14 = pd.concat([df_10_13, df_14], axis = 0, ignore_index=True)\n",
    "df_11_19 = pd.concat([df_10_14, df_19], axis = 0, ignore_index=True)\n",
    "print(df_11_19.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f952d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11_19.to_csv('data/cleaned_projects_raw_11_19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377475c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import joblib\n",
    "import os\n",
    "import nltk\n",
    "# nltk.download('punkt')      # Normal Sentence Segmentation Model\n",
    "# nltk.download('punkt_tab')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b140ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a new copy\n",
    "df = df_11_19.copy()\n",
    "# keep only the non-empty text\n",
    "df = df[df['description'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1dcdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excessive spaces and special characters\n",
    "df['description'] = df['description'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d783e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "df['sentences'] = df['description'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea37751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten all the sentences\n",
    "all_sentences = df['sentences'].explode().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df9db6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d50fdeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached embeddings...\n"
     ]
    }
   ],
   "source": [
    "# encode all sentences (with caching)\n",
    "BATCH_SIZE = 64\n",
    "CACHE_FILE = 'sbert_vectors.pkl'\n",
    "\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    print(\"Loading cached embeddings...\")\n",
    "    sentence_embeddings = joblib.load(CACHE_FILE)\n",
    "else:\n",
    "    print(\"Encoding...\")\n",
    "    sentence_embeddings = model.encode(\n",
    "        all_sentences,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    joblib.dump(sentence_embeddings, CACHE_FILE)\n",
    "    print(\"Embeddings cached!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d48c5101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 184878/227508 [00:01<00:00, 129071.40it/s]e:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "e:\\dissertation\\project\\housing_feature_gentrification\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:137: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "100%|██████████| 227508/227508 [00:01<00:00, 120271.78it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_idx = 0\n",
    "final_vectors = []\n",
    "\n",
    "for sent_list in tqdm(df['sentences']):\n",
    "    count = len(sent_list)\n",
    "    if count == 0:\n",
    "        final_vectors.append(np.zeros(model.get_sentence_embedding_dimension()))\n",
    "    else:\n",
    "        vecs = sentence_embeddings[emb_idx: emb_idx + count]\n",
    "        final_vectors.append(np.mean(vecs, axis=0))\n",
    "        emb_idx += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9394d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame + merge back to main list\n",
    "vec_df = pd.DataFrame(final_vectors)\n",
    "vec_df.columns = [f'sbert_{i}' for i in range(vec_df.shape[1])]\n",
    "df_vectors = pd.concat([df.reset_index(drop=True), vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "292ab3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy\n",
    "df_vectors.to_parquet('sbert_encoded_data_11_19.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "400115d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          site_name decision_date  valid_date  \\\n",
      "0               NaN    2010-05-02  2010-03-08   \n",
      "1  Development Site    2011-03-31  2010-11-30   \n",
      "2               NaN           NaN  2010-04-09   \n",
      "3                38    2011-01-07  2010-11-15   \n",
      "4               NaN    2011-04-28  2010-06-16   \n",
      "\n",
      "                                  polygon.geometries        polygon.type  \\\n",
      "0  [{'coordinates': [[[529321.6973703, 187356.898...  GeometryCollection   \n",
      "1  [{'coordinates': [[[544084.439, 183412.144], [...  GeometryCollection   \n",
      "2                                                NaN                 NaN   \n",
      "3                                                NaN                 NaN   \n",
      "4                                                NaN                 NaN   \n",
      "\n",
      "                           wgs84_polygon.coordinates wgs84_polygon.type  \\\n",
      "0  [[[-0.1353955, 51.5702629], [-0.1352572, 51.57...            Polygon   \n",
      "1  [[[0.075859, 51.5312398], [0.075848, 51.531291...            Polygon   \n",
      "2  [[[-0.087185, 51.544463], [-0.087186, 51.54443...            Polygon   \n",
      "3                                                NaN                NaN   \n",
      "4  [[[-0.121009, 51.53314], [-0.12101, 51.533113]...            Polygon   \n",
      "\n",
      "                                         description               borough  \\\n",
      "0  A loft conversion including raising a section ...             Islington   \n",
      "1  Revised application: Demolition of existing bu...  Barking and Dagenham   \n",
      "2  Tree pruning in the East Canonbury conservatio...             Islington   \n",
      "3                       Single storey rear extension                Ealing   \n",
      "4  Part approval of details pursuant of condition...             Islington   \n",
      "\n",
      "         street_name  ...  sbert_374  sbert_375  sbert_376  sbert_377  \\\n",
      "0     Whitehall Park  ...   0.090720   0.037844   0.027198  -0.015740   \n",
      "1         ABBEY ROAD  ...   0.372778   0.056805  -0.145727   0.069826   \n",
      "2      Ockendon Road  ...   0.318746  -0.203386  -0.166974   0.300346   \n",
      "3  Eastbourne Avenue  ...   0.213875  -0.662830   0.181444   0.390586   \n",
      "4       Balfe Street  ...   0.106714  -0.163149  -0.066535  -0.397721   \n",
      "\n",
      "  sbert_378  sbert_379 sbert_380  sbert_381  sbert_382  sbert_383  \n",
      "0  0.411856   0.302282  0.025305   0.203394  -0.110959   0.223529  \n",
      "1  0.656283   0.150948 -0.417015   0.678698  -0.409876  -0.415659  \n",
      "2  0.219195   0.029909 -0.203221   0.217553  -0.355704   0.459508  \n",
      "3  0.110930   0.200714  0.047162  -0.112153  -0.168227   0.169755  \n",
      "4  0.085191   0.384984  0.314905  -0.135224   0.064136  -0.040522  \n",
      "\n",
      "[5 rows x 401 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_vectors.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
